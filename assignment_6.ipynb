{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "assignment_6.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi07-eng/ML-ASSIGNMENTS/blob/master/assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyhSjiKLKgEN",
        "colab_type": "text"
      },
      "source": [
        "<b><h1>PROJECT :- SMS Spam Collection Data Set</h1></b>\n",
        "<p><h2>Dataset information</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTVKuhaXKgEO",
        "colab_type": "text"
      },
      "source": [
        "This corpus has been collected from free or free for research sources at the Internet:\n",
        "\n",
        "-> A collection of 425 SMS spam messages was manually extracted from the Grumbletext Web site. This is a UK forum in which cell phone users make public claims about SMS spam messages, most of them without reporting the very spam message received. The identification of the text of spam messages in the claims is a very hard and time-consuming task, and it involved carefully scanning hundreds of web pages. The Grumbletext Web site is: [Web Link].\n",
        "-> A subset of 3,375 SMS randomly chosen ham messages of the NUS SMS Corpus (NSC), which is a dataset of about 10,000 legitimate messages collected for research at the Department of Computer Science at the National University of Singapore. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The NUS SMS Corpus is avalaible at: [Web Link].\n",
        "-> A list of 450 SMS ham messages collected from Caroline Tag's PhD Thesis available at [Web Link].\n",
        "-> Finally, we have incorporated the SMS Spam Corpus v.0.1 Big. It has 1,002 SMS ham messages and 322 spam messages and it is public available at: [Web Link]. This corpus has been used in the following academic researches:\n",
        "\n",
        "[1] GÃ³mez Hidalgo, J.M., Cajigas Bringas, G., Puertas Sanz, E., Carrero GarcÃ­a, F. Content Based SMS Spam Filtering. Proceedings of the 2006 ACM Symposium on Document Engineering (ACM DOCENG'06), Amsterdam, The Netherlands, 10-13, 2006.\n",
        "\n",
        "[2] Cormack, G. V., GÃ³mez Hidalgo, J. M., and Puertas SÃ¡nz, E. Feature engineering for mobile (SMS) spam filtering. Proceedings of the 30th Annual international ACM Conference on Research and Development in information Retrieval (ACM SIGIR'07), New York, NY, 871-872, 2007.\n",
        "\n",
        "[3] Cormack, G. V., GÃ³mez Hidalgo, J. M., and Puertas SÃ¡nz, E. Spam filtering for short messages. Proceedings of the 16th ACM Conference on Information and Knowledge Management (ACM CIKM'07). Lisbon, Portugal, 313-320, 2007.\n",
        "\n",
        "\n",
        "\n",
        "**Attribute Information**:\n",
        "\n",
        "The collection is composed by just one text file, where each line has the correct class followed by the raw message. We offer some examples bellow:\n",
        "\n",
        "ham What you doing?how are you?\n",
        "ham Ok lar... Joking wif u oni...\n",
        "ham dun say so early hor... U c already then say...\n",
        "ham MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\n",
        "ham Siva is in hostel aha:-.\n",
        "ham Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.\n",
        "spam FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\n",
        "spam Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B\n",
        "spam URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU\n",
        "\n",
        "Note: the messages are not chronologically sorted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or4_p8YDKgEP",
        "colab_type": "text"
      },
      "source": [
        "Step-1 :- **Getting things ready for the project**\n",
        "           \n",
        "           Import the required libraries\n",
        "           Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tV0saazKgEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Mfgs3uKgEW",
        "colab_type": "code",
        "colab": {},
        "outputId": "b5438e41-6695-46c8-bb48-2f9ddf50ed7b"
      },
      "source": [
        "df=pd.read_csv('SMSSpamCollection',sep='\\t',names=['target','message'])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  target                                            message\n",
              "0    ham  Go until jurong point, crazy.. Available only ...\n",
              "1    ham                      Ok lar... Joking wif u oni...\n",
              "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3    ham  U dun say so early hor... U c already then say...\n",
              "4    ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW_mlTeuKgEa",
        "colab_type": "text"
      },
      "source": [
        "STEP-2 :- **Performing Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7EiC4fzKgEb",
        "colab_type": "code",
        "colab": {},
        "outputId": "6f97c8aa-bed7-4fa2-fcf3-18940537f348"
      },
      "source": [
        "#Performing exploratory data analysis\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   target   5572 non-null   object\n",
            " 1   message  5572 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 87.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy4SnLV2KgEf",
        "colab_type": "code",
        "colab": {},
        "outputId": "3ab082c3-3e74-4813-d66e-7bcee3725908"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK7u_peXKgEl",
        "colab_type": "code",
        "colab": {},
        "outputId": "2bb7560d-2da1-4bbe-fc41-ee4a003b2ce7"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5572</td>\n",
              "      <td>5572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>5169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>ham</td>\n",
              "      <td>Sorry, I'll call later</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>4825</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       target                 message\n",
              "count    5572                    5572\n",
              "unique      2                    5169\n",
              "top       ham  Sorry, I'll call later\n",
              "freq     4825                      30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yFAEhhZKgEp",
        "colab_type": "text"
      },
      "source": [
        "From above perform the Exploratory Data Analysis on the data set I observed that most the target is of ham category.\n",
        "<p>I also be observed that there are 5169 unique message out of total 5572 messages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zAzChP3KgEp",
        "colab_type": "code",
        "colab": {},
        "outputId": "365e6a4c-0b1e-4691-a875-c39eaf68f1e7"
      },
      "source": [
        "df.target.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ham     0.865937\n",
              "spam    0.134063\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99UqC4inKgEs",
        "colab_type": "text"
      },
      "source": [
        "From above I notice that there is 13.4% of the given data is **spam**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw5LQnVaKgEt",
        "colab_type": "code",
        "colab": {},
        "outputId": "cd5cfe07-63f8-4534-afc2-42b4ae196d09"
      },
      "source": [
        "sns.countplot(x='target',data=df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x24c1fb87ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCOFi4OmKgEv",
        "colab_type": "text"
      },
      "source": [
        "STEP-3 :- **Preprocess the Data**\n",
        "               \n",
        "               Remove special characters\n",
        "               Conversion to lowercase\n",
        "               Tokenization\n",
        "               Stemming or Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTnlrVr4KgEw",
        "colab_type": "code",
        "colab": {},
        "outputId": "a5c0bfbd-95f8-4e57-a467-61dfbc8d11f9"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Devendra\n",
            "[nltk_data]     Solanki\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\Devendra\n",
            "[nltk_data]     Solanki\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRHscnh3KgEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "# # We can also use Lemmatizer instead of Stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# initializing the lists\n",
        "\n",
        "clean_lst = []\n",
        "\n",
        "len_lst = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZYQEJkaKgE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(raw_data, flag):\n",
        "    # Removing special characters and digits\n",
        "    special_letters = re.sub(\"[^a-zA-Z]\", \" \",raw_data)\n",
        "    \n",
        "    # Change sentence to lower case\n",
        "    special_letters = letters_only.lower()\n",
        "\n",
        "    # Tokenize into words\n",
        "    words = special_letters.split()\n",
        "    \n",
        "    # Remove stop words                \n",
        "    words = [w for w in words if not w in stopwords.words(\"english\")]\n",
        "    \n",
        "    #Stemming/Lemmatization\n",
        "    if(flag == 'stem'):\n",
        "        words = [stemmer.stem(word) for word in words]\n",
        "    else:\n",
        "        words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    \n",
        "    clean_lst.append(\" \".join(words))\n",
        "    \n",
        "    len_lst.append(len(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MRFuDZdKgE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "tqdm.pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyvbn_EiKgE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_lst = []\n",
        "\n",
        "len_lst = []\n",
        "\n",
        "df['message'].apply(lambda x: preprocess(x, 'stem'))\n",
        "\n",
        "df['clean_stem'] = clean_lst\n",
        "\n",
        "df['stem_length'] = len_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fYcseAYKgE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_lst = []\n",
        "\n",
        "len_lst = []\n",
        "\n",
        "df['message'].apply(lambda x: preprocess(x, 'lemma'))\n",
        "\n",
        "df['clean_lemma'] = clean_lst\n",
        "\n",
        "df['lemma_length'] = len_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xASCDIUZKgE-",
        "colab_type": "code",
        "colab": {},
        "outputId": "526b33ff-5157-4a0f-a13f-d255a37bb508"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>message</th>\n",
              "      <th>clean_stem</th>\n",
              "      <th>stem_length</th>\n",
              "      <th>clean_lemma</th>\n",
              "      <th>lemma_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>go jurong point crazi avail bugi n great world...</td>\n",
              "      <td>16</td>\n",
              "      <td>go jurong point crazy available bugis n great ...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ok lar joke wif u oni</td>\n",
              "      <td>6</td>\n",
              "      <td>ok lar joking wif u oni</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>free entri wkli comp win fa cup final tkt st m...</td>\n",
              "      <td>21</td>\n",
              "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>u dun say earli hor u c alreadi say</td>\n",
              "      <td>9</td>\n",
              "      <td>u dun say early hor u c already say</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>nah think goe usf live around though</td>\n",
              "      <td>7</td>\n",
              "      <td>nah think go usf life around though</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  target                                            message  \\\n",
              "0    ham  Go until jurong point, crazy.. Available only ...   \n",
              "1    ham                      Ok lar... Joking wif u oni...   \n",
              "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
              "3    ham  U dun say so early hor... U c already then say...   \n",
              "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
              "\n",
              "                                          clean_stem  stem_length  \\\n",
              "0  go jurong point crazi avail bugi n great world...           16   \n",
              "1                              ok lar joke wif u oni            6   \n",
              "2  free entri wkli comp win fa cup final tkt st m...           21   \n",
              "3                u dun say earli hor u c alreadi say            9   \n",
              "4               nah think goe usf live around though            7   \n",
              "\n",
              "                                         clean_lemma  lemma_length  \n",
              "0  go jurong point crazy available bugis n great ...            16  \n",
              "1                            ok lar joking wif u oni             6  \n",
              "2  free entry wkly comp win fa cup final tkts st ...            21  \n",
              "3                u dun say early hor u c already say             9  \n",
              "4                nah think go usf life around though             7  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ujqyOe95KgFA",
        "colab_type": "code",
        "colab": {},
        "outputId": "6db7ad43-45f0-46ee-9f76-50d12598aff0"
      },
      "source": [
        "# Preparing the target variable\n",
        "\n",
        "df['target_num']=df['target'].apply(lambda x: 0 if x=='ham' else 1)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>message</th>\n",
              "      <th>clean_stem</th>\n",
              "      <th>stem_length</th>\n",
              "      <th>clean_lemma</th>\n",
              "      <th>lemma_length</th>\n",
              "      <th>target_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>go jurong point crazi avail bugi n great world...</td>\n",
              "      <td>16</td>\n",
              "      <td>go jurong point crazy available bugis n great ...</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ok lar joke wif u oni</td>\n",
              "      <td>6</td>\n",
              "      <td>ok lar joking wif u oni</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>free entri wkli comp win fa cup final tkt st m...</td>\n",
              "      <td>21</td>\n",
              "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
              "      <td>21</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>u dun say earli hor u c alreadi say</td>\n",
              "      <td>9</td>\n",
              "      <td>u dun say early hor u c already say</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>nah think goe usf live around though</td>\n",
              "      <td>7</td>\n",
              "      <td>nah think go usf life around though</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  target                                            message  \\\n",
              "0    ham  Go until jurong point, crazy.. Available only ...   \n",
              "1    ham                      Ok lar... Joking wif u oni...   \n",
              "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
              "3    ham  U dun say so early hor... U c already then say...   \n",
              "4    ham  Nah I don't think he goes to usf, he lives aro...   \n",
              "\n",
              "                                          clean_stem  stem_length  \\\n",
              "0  go jurong point crazi avail bugi n great world...           16   \n",
              "1                              ok lar joke wif u oni            6   \n",
              "2  free entri wkli comp win fa cup final tkt st m...           21   \n",
              "3                u dun say earli hor u c alreadi say            9   \n",
              "4               nah think goe usf live around though            7   \n",
              "\n",
              "                                         clean_lemma  lemma_length  target_num  \n",
              "0  go jurong point crazy available bugis n great ...            16           0  \n",
              "1                            ok lar joking wif u oni             6           0  \n",
              "2  free entry wkly comp win fa cup final tkts st ...            21           1  \n",
              "3                u dun say early hor u c already say             9           0  \n",
              "4                nah think go usf life around though             7           0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnKhFE4vKgFD",
        "colab_type": "text"
      },
      "source": [
        "STEP-4 :- **Data Preparation**\n",
        "\n",
        "               Train Test Split\n",
        "               Identify the target variable\n",
        "               Use Bag of Words to convert the Text data to Numerical Vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO2XIjTdKgFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting into test and train\n",
        "\n",
        "from sklearn.model_selection  import train_test_split\n",
        "\n",
        "train, test = train_test_split(df,test_size=0.2,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TnBlvnNKgFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_clean_data=[]\n",
        "for msg in train['clean_stem']:\n",
        "    train_clean_data.append(msg)\n",
        "\n",
        "test_clean_data=[]\n",
        "for msg in test['clean_stem']:\n",
        "    test_clean_data.append(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiiXht_rKgFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer = \"word\")\n",
        "\n",
        "train_features = vectorizer.fit_transform(train_clean)\n",
        "\n",
        "test_features = vectorizer.transform(test_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8Sg4FQXKgFK",
        "colab_type": "code",
        "colab": {},
        "outputId": "041b9c91-9e1d-413e-9531-7c0aa5f31645"
      },
      "source": [
        "print(\"Total unique words:\", len(vectorizer.vocabulary_))\n",
        "\n",
        "print(\"Type of train_features:\", type(train_features))\n",
        "\n",
        "print(\"Shape of input data\", train_features.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total unique words: 5593\n",
            "Type of train_features: <class 'scipy.sparse.csr.csr_matrix'>\n",
            "Shape of input data (4457, 5593)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksV9txvHKgFM",
        "colab_type": "text"
      },
      "source": [
        "STEP-5 :- **Model Building and evaluation**\n",
        "               \n",
        "          \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2w286EvKgFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YIMaf8qKgFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Classifiers = [\n",
        "    LogisticRegression(),\n",
        "    DecisionTreeClassifier(),\n",
        "    SVC()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U_cNj1rKgFQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "2afcec88-6d65-4938-d0f8-f201ba3b3bb5"
      },
      "source": [
        "abhi = train_features.toarray()\n",
        "\n",
        "mat = test_features.toarray()\n",
        "\n",
        "for classifier in tqdm(Classifiers):\n",
        "    fit = classifier.fit(abhi,train['target_num'])\n",
        "    predict = fit.predict(mat)\n",
        "    accuracy = accuracy_score(pred,test['target_num'])\n",
        "    print('Accuracy of '+classifier.__class__.__name__+' is '+str(accuracy))\n",
        "    con_metric=metrics.confusion_matrix(test['target_num'],pred)\n",
        "    print(con_metric)\n",
        "    print(metrics.classification_report(test['target_num'],pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of LogisticRegression is 0.9838565022421525\n",
            "[[965   1]\n",
            " [ 17 132]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 33%|████████████████████████████                                                        | 1/3 [00:04<00:08,  4.33s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       966\n",
            "           1       0.99      0.89      0.94       149\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.99      0.94      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            " 67%|████████████████████████████████████████████████████████                            | 2/3 [02:10<00:40, 40.91s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of DecisionTreeClassifier is 0.9713004484304932\n",
            "[[954  12]\n",
            " [ 20 129]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       966\n",
            "           1       0.91      0.87      0.89       149\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.95      0.93      0.94      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [03:45<00:00, 75.05s/it]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of SVC is 0.9838565022421525\n",
            "[[966   0]\n",
            " [ 18 131]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       966\n",
            "           1       1.00      0.88      0.94       149\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.99      0.94      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}